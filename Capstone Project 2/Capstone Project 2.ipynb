{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can We Predict the Weather Accurately Using Neural Networks?\n",
    "\n",
    "\n",
    "<b><u>Abstract:</u></b><br>\n",
    "Predicting the weather has been of importance to humans for thousands of years; it dictates how we make a multitude of decisions, varying from choosing what to wear in the morning to knowing when is a good time to plant crops. Currently, our weather predictions lie in the hands of meteorologists who use complicated models and algorithms based on concepts from physics, chemistry and mathematics in order to make their predictions. While the quality of these predictions has steadily improved over the years, the mathematically chaotic nature that weather patterns tend to follow make predictions extremely difficult, especially over long periods of time. Improving the accuracy of these predictions using machine learning models could be the next step towards advanced weather predictions. The work presented here shows that by using a model consisting of a combination of different nerual networks, reasonable predictions can be made. The quality of these predictions is perhaps hinting at the possibility of more accuracte models that could be developed in the near future. \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img width=300 height=300 src=\"images/neural_network_brain.png\">\n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>     \n",
    "                <img src=\"images/weather_forcast.jpg\">\n",
    "           </figure>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0: Imports, Functions, and Global Variables\n",
    "As its name suggests, this section contains all of the imports, functions, and global variable declations that are used in order to complete this project. All of these items were included here to enhance readability of later sections and provide a single place where they can be referenced, if necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1: Imports\n",
    "Below is a list of all imports that were used in order to complete this project, organized by category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(91993) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting imports \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical tools imports\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.integrate import odeint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing Imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Algorithm Imports\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection (hyperparameter tuning) imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network Imports (Keras and tensorflow)\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2: Functions\n",
    "Below is a list of all functions that were used in order to complete this project, listed in the order in which they are first used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_classes(rainfall, snowfall):\n",
    "    '''Creates target variable to predict by generating the following three classes from \n",
    "    rainfall and snowfall measurements:\n",
    "    \n",
    "    0 --> Clear weather, no rain or snow.\n",
    "    1 --> Rainy weather.\n",
    "    2 --> Snowy weather. \n",
    "    '''\n",
    "    if snowfall == 0:\n",
    "        if rainfall == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return 2\n",
    "    \n",
    "def make_classes_extra(rainfall, snowfall):\n",
    "    '''Same as the 'make_classes' function, but adds an additional two classes based on the \n",
    "    severity of rainfall and snowdfall. Classes are as follows: \n",
    "    \n",
    "    0 --> Clear weather, no rain or snow.\n",
    "    1 --> Rain less than 0.3mm in the hour. Considered light rainfall.\n",
    "    2 --> Rain more than 0.3mm in the hour. Considered moderate to heavy rainfall.\n",
    "    3 --> Snow less than 0.2cm in the hour. Considered light snowfall.\n",
    "    4 --> Snow more than 0.2cm in the hour. Considered moderate to heavy snowfall.\n",
    "    '''\n",
    "    if snowfall ==  0:\n",
    "        if rainfall == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            if rainfall <= 0.3:\n",
    "                return 1\n",
    "            else:\n",
    "                return 2\n",
    "    else:\n",
    "        if snowfall < 0.2:\n",
    "            return 3\n",
    "        else: \n",
    "            return 4\n",
    "    \n",
    "def get_data_ready(dataset):\n",
    "    y_undersample = dataset[dataset['num_hours']==0]['class'].values.reshape(-1,1)\n",
    "    y_undersample_cat = to_categorical(dataset[dataset['num_hours']==0]['class'])\n",
    "    X_undersample = dataset[dataset['num_hours']==0][['temperature', 'humidity', 'mslp', \\\n",
    "                'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample_cat, test_size=0.3, \\\n",
    "                                                        random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3: Global Variables \n",
    "The cell below contains all global variables that were used in this project, in order to increase ease of reproducability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = 42 # The random_state used by all relevant functions and methods in this project. \n",
    "seed_num = 91993 # The seed number for random calculations carried out by the numpy package. \n",
    "np.random.seed(seed_num) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Introduction\n",
    "This sections contains all introductory material that is referenced in later sections of this project. \n",
    "### 1.1: How are Weather Predictions Made?\n",
    "Weather predictions are something that all of us rely on every day. In fact, the first thing most people do before heading out the door is check their favorite weather app to determine what they should be wearing that day. While we are surrounding by weather predictions every day, it turns out that the models used to calculate these predictions are extemeley complex. While there are a multitude of different methods for doing this, they almost always rely on some implementation of the Navier-Stokes equations. These equations are the foundation of fluid dynamics, similar to how the Schrodinger equation is the foundation of quantum mechanics. While these equaitons are a great achievement of physics, the fact that they are intricate differential equations makes them difficult to solve. One version of the equations can be seen below, provided by NASA: \n",
    "\n",
    "<img width=700 height=700 src=\"images/navier_stokes_equations.gif\"/>\n",
    "\n",
    "While the equations do provide information about the temperature, pressure, momentum, and density of a moving fluid, there is a lot of computational work that needs to be done in order to solve them. <br>\n",
    "\n",
    "In addition to the complex nature of the Navier Stokes equations, weather systems are also considered to be chaotic systems. Mathematically speaking, chaotic systems exhibit the following traits:\n",
    "\n",
    "<ol>\n",
    "    <li> The system is deterministic.</li>\n",
    "    <li> The system is aperiodic.</li>\n",
    "    <li> The system is extremely sensitive to intitial conditions.</li>\n",
    "</ol>\n",
    "        \n",
    "The first trait might not make sense at first, as it might seem that a chaotic system is one that shouldn't be able to be perfectly described (deterministic). However, this is a common misconception: chaotic systems are entirely predictable, even for long periods of time, if you have the correct inputs. The real difficulty in modeling chaotic systems results from the third trait: chaotic systems are extremeley sensitive to initial conditions, meaning that two slightly different sets of inputs can result is drastically different predictions. This means that no matter how close a meterologist is to getting spot on atmospheric data, even the smallest discrepensy from the actual model can cause predictions to be way off in the near future. This is sometimes more colloquially referred to as the \"butterfly effect.\" The following aside gives an example of a more simplistic chaotic system. \n",
    "\n",
    "### Aside: A Simple Chaotic System - The Lorenz System\n",
    "One of the very first systems that is studied in any chaos theory class in known as the Lorenz system, first studied in detail by Edward Lorenz. This system relates to the project presented here, as he studied it in an attempt to better understand atmospheric convection. The system consists of three ordinary differential equations, which are shown below:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{dx}{dt} &= \\sigma(y-x) \\\\\n",
    "\\frac{dy}{dt} &= x(\\rho-z)-x \\\\\n",
    "\\frac{dz}{dt} &= xy - \\beta z\n",
    "\\end{align}\n",
    "$$\n",
    "In the equations above, $x$, $y$, and $z$ are cartesian coordinates, while $\\sigma$, $\\rho$, and $\\beta$ are system parameters. This system is translated into python in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dx_dt(x, y):\n",
    "    return sigma * (y - x)\n",
    "def dy_dt(x, z):\n",
    "    return x * (rho - z) - x\n",
    "def dz_dt(x, y, z):\n",
    "    return x * y - beta * z\n",
    "def lorenz_system(coordinates, t):\n",
    "    x, y, z = coordinates\n",
    "    return dx_dt(x, y), dy_dt(x, z), dz_dt(x, y, z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that for most values of $\\sigma$, $\\rho$, and $\\beta$, the Lorenz system is not chaotic at all. More specifically, if $\\rho$ is small, the system almost always converges to one or two fixed points. However, once $\\rho$ is larger than $\\approx$ 24.74, chaotic behavior starts to occur. Feel free to change the intial conditions ($\\sigma$, $\\rho$, and $\\beta$) in the cell below in order to see how the behavior of the lorenz system changes. Some suggestions are given below:\n",
    "<ul>\n",
    "    <li>$\\rho$=10, $\\sigma$=10, $\\beta=\\frac{8}{3}$: To see how the lorenz system behaves when it is not chaotic.</li>\n",
    "    <li>$\\rho$=28, $\\sigma$=10, $\\beta=\\frac{8}{3}$: To see how the lorenz system behaves when it is not chaotic.</li>\n",
    "    <li>$\\rho$=28.1, $\\sigma$=10, $\\beta=\\frac{8}{3}$: To see how a small change to a chaotic system produces an entirely different result.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing these three variables\n",
    "rho = 10.0\n",
    "sigma = 10.0\n",
    "beta = 8.0 / 3.0\n",
    "\n",
    "# Do not change these...\n",
    "t_i = 0\n",
    "t_f = 20\n",
    "t_step = 0.01\n",
    "t = np.arange(t_i, t_f, t_step)\n",
    "initial_xyz = [1.0, 1.0, 1.0]\n",
    "\n",
    "def plot_lorenz(initial_xyz, t):\n",
    "    positions = odeint(lorenz_system, initial_xyz, t)\n",
    "    plt.axes(projection='3d')\n",
    "    plt.title('Lorenz System')\n",
    "    plt.plot(positions[:,0], positions[:,1], positions[:,2])\n",
    "    \n",
    "plt.figure(figsize=(12,10))\n",
    "plot_lorenz(initial_xyz, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case its too hard to make out the difference, the cell below shows the Lorenz system when $\\rho$=28.0, $\\sigma$=10.0, $\\beta=\\frac{8}{3}$, and when $\\rho$=28.1, $\\sigma$=10.0, $\\beta=\\frac{8}{3}$.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <figure>\n",
    "                <img src=\"images/lorenz1.png\">\n",
    "                <figcaption><center>Initial Conditions: $\\rho$=28.0, $\\sigma$=10.0, and $\\beta=\\frac{8}{3}$.</center></figcaption>   \n",
    "            </figure>\n",
    "        </td>\n",
    "        <td>\n",
    "            <figure>     \n",
    "                <img src=\"images/lorenz2.png\">\n",
    "                <figcaption><center>Initial Conditions: $\\rho$=28.1, $\\sigma$=10.0, and $\\beta=\\frac{8}{3}$.</center></figcaption>\n",
    "           </figure>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the general shape of the graph is the same, further inspection reveals many differences by simply shifting $\\rho$ by only 0.1 Another way of looking at this is to examine the final points of the lorenz system using each set of initial conditions. This is done in the cell below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 28.0\n",
    "sigma = 10.0\n",
    "beta = 8.0 / 3.0\n",
    "positions1 = odeint(lorenz_system, initial_xyz, t)\n",
    "rho = 28.1\n",
    "sigma = 10.0\n",
    "beta = 8.0 / 3.0\n",
    "positions2 = odeint(lorenz_system, initial_xyz, t)\n",
    "print('Last Positions of Lorenz system with rho = 28.0: ', positions1[-1])\n",
    "print('Last Positions of Lorenz system with rho = 28.1: ', positions2[-1])\n",
    "print(mean_squared_error(positions1, positions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the final points from each system are completely different despite their initial conditions being nearly identical. Lastly, we can look at the mean squared error between the points from the two different Lorenz systems that were calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Squared Error between the two lorenz systems:', mean_squared_error(positions1, positions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear from the printout above, despite their shape looking similar, the points from the two different systems have an exorbitantly high mean squared error.\n",
    "\n",
    "The point in showing this is to demonstrate that this sensitivity to inital conditions for chaotic systems can make correct predictions extremely difficult. The systems that dictate our weather patterns are also much more complex, further adding to the issue. In summary, this means is that if the atmospheric data used to make predictions is only slightly different from the actual value, the predictions might not be close to the weather that's actually on its way. This sensitivity also gets worse over time, which accounts for the fact that weather predictions made far in advance have a high probability of being incorrect. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Neural Networks, a Possible Solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous subsection described how the current methods that are used in making weather predictions have some pretty serious flaws. The work performed here is meant to test whether or not predictive algorithms (specifically, neural networks) can be used to make predictions that are as good, or possibly even better than the physical models that are currently in use. <br><br>\n",
    "\n",
    "Nueral networks seemed like a reliable choice to attempt to solve this problem, as they are capable of being able to determine complicated linear and non-linear relationships between a large number of input variables. Because the current method of weather forecasting is also complex, there already exists a large amount of high quality atmospheric data being used to make the predictions as accurate as possible. The work done in this project simply applies that same data to different machine learning models, with the most accurate possible neural network as the desired product. In general, these models performed reasonably well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3: The Data\n",
    "The data used in this project comes from <a href=\"https://www.meteoblue.com/en/weather/week\">meteoblue</a>, a meteorological service created at the University of Basel. They provide current, high quality meteorological data from places all over the world, but also give access to free historical weather data for Basel, Switzerland dating back to 1984. This was the data used for this project, and can be accessed <a href=\"https://www.meteoblue.com/en/products/historyplus/download/basel_switzerland_2661604\">here.</a> A list of all the variables that are included in the dataset, along with their units and a brief description, is listed in the table below:\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <th>Measurement</th>\n",
    "    <th>Units</th> \n",
    "    <th>Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Temperature</td>\n",
    "    <td>$^{\\circ}$F</td> \n",
    "    <td>The temperature recorded from 2 meters above the ground.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Relative humidity</td>\n",
    "    <td>%</td> \n",
    "    <td>the amount of water vapor present in air expressed as a percentage of the amount needed for saturation at the same temperature</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pressure</td>\n",
    "    <td>hPa</td> \n",
    "    <td>The mean sea level pressure (MSLP).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Precipitation Amount</td>\n",
    "    <td>mm</td> \n",
    "    <td>The total amount of precipitation that fell in hour measured according to a high resolution model.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Precipitation amount (low resolution)</td>\n",
    "    <td>mm</td> \n",
    "    <td>The total amount of precipitation that fell in hour measured according to a low resolution model.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Snowfall amount (high resolution)</td>\n",
    "    <td>cm</td> \n",
    "    <td>The total amount of snowdall that fell in the hour measured according to a high resolution model.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Snowfall amount (low resolution)</td>\n",
    "    <td>cm</td> \n",
    "    <td>The total amount of precipitation that fell in the hour measured according to a low resolution model.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Total cloud cover</td>\n",
    "    <td>%</td> \n",
    "    <td>The fraction of the sky covered by a cloud of any type at any height in the sky.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>High cloud cover</td>\n",
    "    <td>%</td> \n",
    "    <td>The fraction of the sky covered by clouds at high elevation (5-13 km).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Mid cloud cover</td>\n",
    "    <td>%</td> \n",
    "    <td>The fraction of the sky covered by clouds at medium elevation (2-7 km).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Low cloud cover</td>\n",
    "    <td>%</td> \n",
    "    <td>The fraction of the sky covered by clouds at low elevation ($<$2 km).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Sunshine duration (minutes)</td>\n",
    "    <td>Minutes</td> \n",
    "    <td>The number of minutes in each hour when there was no cloud cover.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Solar radiation</td>\n",
    "    <td>W/m$^2$</td> \n",
    "    <td>The power emitted by the sun for every sqaure meter. Also referred to as 'irradiance'.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wind speed (10m)</td>\n",
    "    <td>mph</td> \n",
    "    <td>The wind speed measured from 10m above the ground.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wind direction (10m)</td>\n",
    "    <td>$^\\circ$ (0$^{\\circ}$ is North)</td> \n",
    "    <td>The wind direction measured from 10m above the ground.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wind speed (80m)</td>\n",
    "    <td>mph</td> \n",
    "    <td>The wind speed measured from 80m above the ground.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wind direction (80m)</td>\n",
    "    <td>$^{\\circ}$ (0$^{\\circ}$ is North)</td> \n",
    "    <td>The wind direction measured from 10m above the ground.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wind speed (900hPa)</td>\n",
    "    <td>mph</td> \n",
    "    <td>The wind speed measured at an air pressure of 900hPa.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wind direction (900hPa)</td>\n",
    "    <td>$^{\\circ}$ (0$^{\\circ}$ is North)</td> \n",
    "    <td>The wind direction measured at an air pressure of 900hPa.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Wind gusts (10m)</td>\n",
    "    <td>mph</td> \n",
    "    <td>Highest wind speed that was measured in an hour from 10m above the ground. </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Cleaning\n",
    "This section includes all data cleaning steps that were taken in order to prepare the dataset for further analysis. Luckily, not much extensive cleaning was required, given the high quality of data provided by <a href=\"https://www.meteoblue.com/en/weather/week\">meteoblue</a>. The raw data can be accessed from the following <a href=\"basel_weather_7-1-09_to_7-1-19.csv\">file</a>, and contains ten years of hourly atmospheric data from Basel, Switzerland dating from 7/1/09 to 7/1/19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in dataset and convert into a pandas dataframe. \n",
    "df = pd.read_csv('basel_weather_7-1-09_to_7-1-19.csv', sep=';', header=10)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print out dataframe information. \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the prinout above, all variables that are listed in Section 1.3 are present, and none of the columns contain any null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Minute'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all measurements are taken at the top of every hour, the minute column provides no additional information (it is always zero) and can be dropped. This is clear from the `.value_counts()` printout above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop minute column.\n",
    "df = df.drop(columns=['Minute'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Rename the columns so they are clearer and more concise. \n",
    "column_names = ['year', 'month', 'day', 'hour', 'temperature', 'humidity', 'mslp', 'rainfall_hr', 'rainfall_lr', \\\n",
    "                'snowfall_hr', 'snowfall_lr', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']\n",
    "df.columns = column_names\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that both the rainfall and snowfall measurements come in both high resoluton and low resolution options. In order to choose between the two, a direct inquiry to meteoblue was made. According to one of their representatives:\n",
    "\n",
    "<b>\"For precipitation and snowfall amount, we (usually) offer two different datasets:\n",
    "<ol>\n",
    "    <li>The low resolution data, available since 1985, comes from our global NEMS30 (30km resolution) model, and is available for every point on earth.</li>\n",
    "    <li>The high resolution data depends on the location you are collecting the data for. For all regions where we run a local domain of our model, data with a higher resolution is available. On the following <a href=\"https://content.meteoblue.com/en/specifications/data-sources/weather-simulation-data/meteoblue-models\"/>page</a>, you can see which local domain is available for which region and when that it was implemented.</li>\n",
    "</ol>\n",
    "If there is no high resolution domain available, both options will provide the data from NEMS30.\n",
    "We offer these two datasets for precipitation because, opposite to other variables such as temperature, it is not possible to merge two different datasets so that there is no inconsistency remaining.\"</b>\n",
    "\n",
    "The following chart can be obtained by going to the page referenced in the quote above:\n",
    "<img width=700 height=700 src=\"images/meteoblue_data_types.png\"/>\n",
    "Based on this chart, we see that in 2008 meteoblue starting using the high resolution NMM12 model in order to measure rainfall and snowfall amounts in Europe (and thus, Switzerland). Since all the data used in this project is from 2009 or later, using the high resolution data seemed like the better choice. The cell below removes the low resolution data so that only the high resolution data remains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use only the high resolution rainfall and snowfall measurements.\n",
    "df = df.drop(columns = ['rainfall_lr', 'snowfall_lr'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The times that were provided in the dataset were given in Greenwich mean time (GMT), which is two hours behind the time in Basel, Switzerland. The following cell accounts for this difference by shifting the time two hours forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Account for time difference between GMT and time in Basel.\n",
    "df['hour'] = df['hour'].map(lambda x: (x + 2) % 24)\n",
    "for i, row in df.iterrows():\n",
    "      if row.hour == 0 or row.hour == 1:\n",
    "        df.at[i,'day'] += 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>rainfall_hr</b> and <b>snowfall_hr</b> columns give the respective snowfall that occured in the hour after other measurements like temperature and humidity were taken. In the cell below, the values in these columns are used in conjunction with the `make_classes()` function in order to create a new column, called <b>class_0hr</b>, that will act as the target variable in future predictive modeling. The three classes are listed below (which can also be found in the DOCSTRING of the `make_classes()` function):\n",
    "<table style=\"width:30%\">\n",
    "  <tr>\n",
    "    <th>Class</th> \n",
    "    <th>Description</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0</td>\n",
    "    <td>Clear Weather - no rain or snow.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Rainy weather - It rained (but did not snow) any amount in the hour.</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>Snowy weather - It snowed any amount in the hour.</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create classes for the rows.\n",
    "df['class_0hr'] = df.apply(lambda x: make_classes(x.rainfall_hr, x.snowfall_hr), axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe seen above now incldues classes for each of its rows. However, in that current state any model that uses it would only be able to make weather predictions within 1 hour of the other quantities being measured. The cell below adds an additional 12 columns that allow any model to make predictions up to 12 hours in the future. In other words, it changes the dataframe so that each row includes if it was clear, rainy, snowy for 0-12 hours in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 13):\n",
    "    col_name = 'class_' + str(i) + 'hr'\n",
    "    shiftnum = i * -1\n",
    "    df[col_name] = df['class_0hr'].shift(shiftnum)\n",
    "    df = df.dropna()\n",
    "    df[col_name] = df[col_name].astype(int)\n",
    "\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the `.info()` printout above, we now see that each row contains a class that gives the type of weather for 0, 1, 2, all the way up to 12 hours into the future. More specifically, <b>class_0hr</b> says whether it was clear, rainy or snowy between 0-1 hours of the measurements being taken, <b>class_1hr</b> says whther it was clear rainy or snowy between 1-2 hours of the measurements being taken, and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While there are no issues with bad or missing data, the cell below exposes another problem particular to this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "df['class_0hr'].value_counts().plot(kind='bar', title='Count - Class');\n",
    "plt.title('Class Count')\n",
    "plt.xlabel('Class');\n",
    "plt.ylabel('Count');\n",
    "\n",
    "count_class_0, count_class_1, count_class_2 = df['class_0hr'].value_counts()\n",
    "\n",
    "plt.ylim([0, max([count_class_0, count_class_1, count_class_2])*1.15])\n",
    "plt.text(-0.10, count_class_0-1000, count_class_0, fontweight='bold')\n",
    "plt.text(-0.10, count_class_0+1000, 'None', color='orange', fontweight='bold')\n",
    "plt.text(0.90, count_class_1-1000, count_class_1, fontweight='bold')\n",
    "plt.text(0.90, count_class_1+1000, 'Rain', color='orange', fontweight='bold')\n",
    "plt.text(1.90, count_class_2-500, count_class_2, fontweight='bold')\n",
    "plt.text(1.90, count_class_2+1000, 'Snow', color='orange', fontweight='bold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the <b>class_0hr</b> column as an exmaple, it is clear according to the bar graph above that the vast majority of all rows represent hours in which the weather was clear. This can wreck havok on a model's ability to make accurate predicitons, as having discrepencies in the frequency of different classes can cause the model to be more or less likely to choose a certain class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_class_0 = df[df['class_0hr'] == 0]\n",
    "df_class_1 = df[df['class_0hr'] == 1]\n",
    "df_class_2 = df[df['class_0hr'] == 2]\n",
    "\n",
    "count_class_0, count_class_1, count_class_2 = df['class_0hr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_under_0 = df_class_0.sample(count_class_2)\n",
    "df_under_1 = df_class_1.sample(count_class_2)\n",
    "df_undersample = pd.concat([df_under_0, df_under_1, df_class_2], axis=0).reset_index(drop=True)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "df_undersample['class_0hr'].value_counts().plot(kind='bar');\n",
    "plt.title('Class Count Using Undersampling')\n",
    "plt.xlabel('Class');\n",
    "plt.ylabel('Count');\n",
    "\n",
    "count0, count1, count2 = df_undersample['class_0hr'].value_counts()\n",
    "\n",
    "plt.ylim([0, max([count0, count1, count2])*1.15])\n",
    "plt.text(1.95, count0-25, count0, fontweight='bold')\n",
    "plt.text(1.925, count0+25, 'None', color='orange', fontweight='bold')\n",
    "plt.text(0.95, count1-25, count1, fontweight='bold')\n",
    "plt.text(0.925, count1+25, 'Rain', color='orange', fontweight='bold')\n",
    "plt.text(-0.05, count2-25, count2, fontweight='bold')\n",
    "plt.text(-0.075, count2+25, 'Snow', color='orange', fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,50))\n",
    "for i in range(1, 13):\n",
    "    plt.subplot(6, 2, i)\n",
    "    col_name = 'class_' + str(i) + 'hr'\n",
    "    df_undersample[col_name].value_counts().plot(kind='bar')\n",
    "    plt.title('Class Count - %d Hour Ahead' %i)\n",
    "    plt.grid()\n",
    "    plt.ylim([0,3000])\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for i in range(0, 13):\n",
    "    col_name = 'class_' + str(i) + 'hr'\n",
    "    cols = ['year', 'month', 'day', 'hour', 'temperature', 'humidity', 'mslp', 'rainfall_hr', \\\n",
    "            'snowfall_hr', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "            'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "            'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust', col_name]\n",
    "    tmp_df = df[cols]\n",
    "    tmp_df = tmp_df.rename(index=str, columns={col_name: 'class'})\n",
    "    df_class_0 = tmp_df[tmp_df['class'] == 0]\n",
    "    df_class_1 = tmp_df[tmp_df['class'] == 1]\n",
    "    df_class_2 = tmp_df[tmp_df['class'] == 2]\n",
    "    \n",
    "    count_class_0, count_class_1, count_class_2 = tmp_df['class'].value_counts()\n",
    "    df_under_0 = df_class_0.sample(count_class_2)\n",
    "    df_under_1 = df_class_1.sample(count_class_2)\n",
    "    class_df = pd.concat([df_under_0, df_under_1, df_class_2], axis=0).reset_index(drop=True)\n",
    "    class_df['num_hours'] = i\n",
    "    df_list.append(class_df)\n",
    "\n",
    "df_us = pd.concat(df_list).reset_index(drop=True)\n",
    "df_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_for_hist = ['temperature', 'humidity', 'mslp', 'rainfall_hr', \\\n",
    "                'snowfall_hr', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']\n",
    "plt.figure(figsize=(15,60))\n",
    "plotnum = 1\n",
    "for column in cols_for_hist:\n",
    "    plt.subplot(len(cols_for_hist)/2, 2, plotnum)\n",
    "    plotnum += 1\n",
    "    plt.hist(x=df_us[df_us['num_hours']==0][column], bins='auto')\n",
    "    plt.grid()\n",
    "    plt.title('Histogram - {}'.format(column), fontweight='bold')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_scatter_cols = ['temperature', 'humidity', 'mslp', \\\n",
    "                 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                 'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                 'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']\n",
    "y_scatter_cols = ['rainfall_hr', 'snowfall_hr']\n",
    "plt.figure(figsize=(15,120))\n",
    "plotnum = 1\n",
    "for x_col in x_scatter_cols:\n",
    "    for y_col in y_scatter_cols:\n",
    "        plt.subplot(len(x_scatter_cols), len(y_scatter_cols), plotnum)\n",
    "        plotnum +=1\n",
    "        r = pearsonr(x=df_us[df_us['num_hours']==0][x_col], y=df_us[df_us['num_hours']==0][y_col])[0]\n",
    "        plt.scatter(x=df_us[df_us['num_hours']==0][x_col], y=df_us[df_us['num_hours']==0][y_col], \\\n",
    "                   label='Pearson Correlation Coefficient (r) = {}'.format(r))\n",
    "        plt.legend()\n",
    "        plt.ylim([0, max(df_us[df_us['num_hours']==0][y_col])*1.15])\n",
    "        plt.grid()\n",
    "        plt.title('Scatterplot - %s vs. %s' %(x_col, y_col), fontweight='bold')\n",
    "        plt.ylabel(y_col)\n",
    "        plt.xlabel(x_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_undersample = df_us[df_us['num_hours']==0]['class'].values\n",
    "y_undersample_cat = to_categorical(df_us[df_us['num_hours']==0]['class'])\n",
    "X_undersample = df_us[df_us['num_hours']==0][['temperature', 'humidity', 'mslp', \\\n",
    "                'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']].values\n",
    "\n",
    "n_inputs = X_undersample.shape[1]\n",
    "n_outputs = len(df_us[df_us['num_hours']==0]['class'].unique())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample, test_size=0.3, \\\n",
    "                                                            random_state=rs)\n",
    "X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_undersample, y_undersample_cat, \\\n",
    "                                                                            test_size=0.3, random_state=rs)\n",
    "\n",
    "X_undersample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "max_n = 15\n",
    "neighbors = np.arange(1, max_n+1)\n",
    "train_scores = np.empty(len(neighbors))\n",
    "test_scores = np.empty(len(neighbors))\n",
    "for i, k in enumerate(neighbors):\n",
    "    knn_tmp = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn_tmp.fit(X_train, y_train)\n",
    "    train_scores[i] = knn_tmp.score(X_train, y_train)\n",
    "    test_scores[i] = knn_tmp.score(X_test, y_test)\n",
    "\n",
    "plt.plot(neighbors, train_scores, label = 'Training Set Performance')\n",
    "plt.plot(neighbors, test_scores, label = 'Test Set Performance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Neighbors (n)')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('K-Nearest Neighbor Performace (No Preprocessing)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_score = max(test_scores)\n",
    "best_k = neighbors[np.where(test_scores == best_test_score)][0]\n",
    "print('K-Nearest Neighbor Model Results (No Preprocessing):\\n')\n",
    "print('Best Test Data Performance: %3.2f' %best_test_score)\n",
    "print('Optimal Number of Neighbors: %d' % best_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "r = pearsonr(df_undersample.humidity, df_undersample.temperature)[0]\n",
    "plt.scatter(x=df_undersample.humidity, y=df_undersample.temperature, label =\\\n",
    "         'Pearson Correlation Coefficient: %2.2f' %r)\n",
    "plt.title('Scatterplot - Temperature vs. Humidity')\n",
    "pearsonr(df_undersample.humidity, df_undersample.temperature)[0]\n",
    "plt.xlabel('Humidty %')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylim([0, max(df_undersample.temperature)*1.15])\n",
    "plt.ylabel('Temperature (F)');\n",
    "#pearsonr(df_undersample.humidity, df_undersample.temperature)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "pipeline.fit(X_undersample)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.title('Variance of PCA Features', fontweight='bold')\n",
    "plt.xlabel('PCA Feature')\n",
    "plt.ylabel('Variance')\n",
    "plt.grid()\n",
    "plt.xticks(features)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(features, np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.title('Percentage of Variance Explained By Different Numbers of PCA Features', fontweight='bold')\n",
    "plt.xlabel('PCA Feature')\n",
    "plt.axhline(y=0.95, color = 'red', linestyle='--', label='95% Boundary')\n",
    "plt.legend()\n",
    "plt.ylabel('Fraction of Explained Variance')\n",
    "plt.grid()\n",
    "plt.xticks(features);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "best_n = 0\n",
    "for percent in pca.explained_variance_ratio_:\n",
    "    total += percent\n",
    "    best_n += 1\n",
    "    if total >= 0.95:\n",
    "        break\n",
    "\n",
    "print('%d PCA Features explain %2.2f %% of the variance.' %(best_n, total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=best_n)\n",
    "pipeline = make_pipeline(scaler, pca)\n",
    "pca_features = pipeline.fit_transform(X_undersample)\n",
    "target = df_us[df_us['num_hours']==0]['class'].values\n",
    "\n",
    "pca_X_train, pca_X_test, y_train, y_test = train_test_split(pca_features, target, test_size=0.3, \\\n",
    "                                                            random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "max_n = 15\n",
    "neighbors = np.arange(1, max_n+1)\n",
    "train_scores = np.empty(len(neighbors))\n",
    "test_scores = np.empty(len(neighbors))\n",
    "for i, k in enumerate(neighbors):\n",
    "    knn_tmp = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn_tmp.fit(pca_X_train, y_train)\n",
    "    train_scores[i] = knn_tmp.score(pca_X_train, y_train)\n",
    "    test_scores[i] = knn_tmp.score(pca_X_test, y_test)\n",
    "\n",
    "plt.plot(neighbors, train_scores, label = 'Training Set Performance')\n",
    "plt.plot(neighbors, test_scores, label = 'Test Set Performance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Neighbors (n)')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.title('K-Nearest Neighbor Performance (with Preprocessing)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_score = max(test_scores)\n",
    "best_k = neighbors[np.where(test_scores == best_test_score)]\n",
    "print('K-Nearest Neighbor Model Results (with Preprocessing):\\n')\n",
    "print('Best Test Data Performance: %3.2f' %best_test_score)\n",
    "print('Optimal Number of Neighbors: %d' % best_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(0, 13):\n",
    "    subset = df_us[df_us['num_hours']==i]\n",
    "    y = subset['class'].values\n",
    "    X = subset.drop(columns=['num_hours', 'class', 'rainfall_hr', 'snowfall_hr', \\\n",
    "                             'year', 'month', 'day', 'hour']).values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA()\n",
    "    pipeline = make_pipeline(scaler, pca)\n",
    "    pipeline.fit(X)\n",
    "    best_n = 0\n",
    "    total = 0\n",
    "    for percent in pca.explained_variance_ratio_:\n",
    "        total += percent\n",
    "        best_n += 1\n",
    "        if total >= 0.95:\n",
    "            break\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA(n_components=best_n)\n",
    "    pipeline = make_pipeline(scaler, pca)\n",
    "    X_transform = pipeline.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transform, y, test_size=0.3, random_state=rs)\n",
    "    \n",
    "    max_n = 15\n",
    "    neighbors = np.arange(1, max_n+1)\n",
    "    train_scores = np.empty(len(neighbors))\n",
    "    test_scores = np.empty(len(neighbors))\n",
    "    for j, k in enumerate(neighbors):\n",
    "        knn_tmp = KNeighborsClassifier(n_neighbors = k)\n",
    "        knn_tmp.fit(X_train, y_train)\n",
    "        train_scores[j] = knn_tmp.score(X_train, y_train)\n",
    "        test_scores[j] = knn_tmp.score(X_test, y_test)\n",
    "    best_test_score = max(test_scores)\n",
    "    best_train_score = max(train_scores)\n",
    "   \n",
    "    tmp.append([i, best_train_score, best_test_score])\n",
    "\n",
    "tmp = np.asarray(tmp)\n",
    "KNN_performances_df = pd.DataFrame({'num_hours': tmp[:,0], 'best_train_performance': tmp[:,1],\\\n",
    "                                        'best_test_performance': tmp[:,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(KNN_performances_df.num_hours, KNN_performances_df.best_test_performance)\n",
    "plt.ylim([0.5, 1])\n",
    "plt.grid()\n",
    "plt.xlabel('Number of Hours to Prediction')\n",
    "plt.ylabel('Best KNN Test Set Score')\n",
    "\n",
    "plt.title('KNN Performances at Different Prediction Time Intervals');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample, test_size=0.3, \\\n",
    "                                                    random_state=rs)\n",
    "\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "print('Out of the box score for Random Forest Classifier: {}'.format(rfc.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2, 10, 100],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'min_samples_leaf': np.arange(2, 6),\n",
    "    'min_samples_split': np.arange(2, 6)\n",
    "}\n",
    "\n",
    "\n",
    "gscv = GridSearchCV(rfc, param_grid, cv=3)\n",
    "gscv.fit(X_train, y_train)\n",
    "\n",
    "print(gscv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rfc.feature_importances_, \\\n",
    "                                   index = df_us[df_us['num_hours']==0].drop(columns=['class', 'num_hours', 'year', 'month', 'day', 'hour', 'rainfall_hr', 'snowfall_hr']).columns, \\\n",
    "                                   columns=['importance']).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.barh(feature_importances.index, feature_importances.importance.values)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature Name')\n",
    "plt.title('Feature Importance Using Random Forest Classifer');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(pca_X_train, y_train)\n",
    "predictions = svc.predict(pca_X_test)\n",
    "svc.score(pca_X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'C': np.logspace(-6, 6, 13),\n",
    "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'gamma': np.logspace(-6, 6, 13),\n",
    "              'tol': np.logspace(-6, 6, 13)}\n",
    "\n",
    "#svc = SVC()\n",
    "#best_svc = RandomizedSearchCV(svc, param_dist, cv=3)\n",
    "#best_svc.fit(pca_X_train, y_train)\n",
    "#print(\"Tuned SVC Parameters: {}\".format(best_svc.best_params_))\n",
    "#print(\"Best score is {}\".format(best_svc.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_svc.score(pca_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: In-Depth Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_undersample = df_us[df_us['num_hours']==0]['class'].values.reshape(-1,1)\n",
    "y_undersample_cat = to_categorical(df_us[df_us['num_hours']==0]['class'])\n",
    "X_undersample = df_us[df_us['num_hours']==0][['temperature', 'humidity', 'mslp', \\\n",
    "                'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample_cat, test_size=0.3, \\\n",
    "                                                    random_state=rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "X_scaled_training = X_scaler.fit_transform(X_train)\n",
    "X_scaled_testing = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('inupt'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, num_inputs))\n",
    "\n",
    "with tf.variable_scope('layer1'):\n",
    "    weights = tf.get_variable(name='weights1', shape=[num_inputs, layer_1_nodes], \\\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_1_out = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "with tf.variable_scope('layer2'):\n",
    "    weights = tf.get_variable(name='weights2', shape=[layer_1_nodes, layer_2_nodes], \\\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_out = tf.nn.relu(tf.matmul(layer_1_out, weights) + biases)\n",
    "\n",
    "with tf.variable_scope('layer3'):\n",
    "    weights = tf.get_variable(name='weights3', shape=[layer_2_nodes, layer_3_nodes], \\\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_out = tf.nn.relu(tf.matmul(layer_2_out, weights) + biases)\n",
    "\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name='weights4', shape=[layer_3_nodes, num_outputs], \\\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\", shape=[num_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.softmax(tf.matmul(layer_3_out, weights) + biases)\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "    cost = tf.losses.softmax_cross_entropy(Y, prediction)\n",
    "\n",
    "# define optimzier\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter('./logs/training', session.graph)\n",
    "    testing_writer = tf.summary.FileWriter('./logs/testing', session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_scaled_training, Y: y_train})\n",
    "\n",
    "         # Every few training steps, log our progress\n",
    "        if epoch % display_step == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y:y_train})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y:y_test})\n",
    "\n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "\n",
    "            # Print the current training status to the screen\n",
    "            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "    # Training is now complete!\n",
    "\n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: y_train})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: y_test})\n",
    "\n",
    "    print(\"Final Training cost: {}\".format(final_training_cost))\n",
    "    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
    "\n",
    "    y_predicted = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "    \n",
    "    inputs = {\n",
    "        'input': tf.saved_model.utils.build_tensor_info(X)\n",
    "        }\n",
    "    outputs = {\n",
    "        'earnings': tf.saved_model.utils.build_tensor_info(prediction)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 14\n",
    "y_test_not_one_hot = np.argmax(y_test, axis=1)\n",
    "if y_test_not_one_hot[i] == 0:\n",
    "    weather = 'Clear'\n",
    "elif y_test_not_one_hot[i] == 1:\n",
    "    weather = 'Rain'\n",
    "else:\n",
    "    weather = 'Snow'\n",
    "\n",
    "print('Sample Prediction from Neural Network:')\n",
    "print('Percent Chance of Clear Weather: {}'.format(y_predicted[i][0]*100))\n",
    "print('Percent Chance of Rain: {}'.format(y_predicted[i][1]*100))\n",
    "print('Percent Chance of Snow: {}'.format(y_predicted[i][2]*100))\n",
    "print('\\n\\nAcutal Weather: %s' %weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted_not_one_hot = np.argmax(y_predicted, axis=1) \n",
    "print('Neural Network Test Set Accuracy: {}'\\\n",
    "      .format(np.sum((y_predicted_not_one_hot == y_test_not_one_hot) / len(y_test_not_one_hot))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define structure of model\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape=(n_inputs, )))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compile model\n",
    "adam = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X_scaled_training, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_scaled_testing)\n",
    "print(model.evaluate(X_scaled_training, y_train)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_predicted_not_one_hot = np.argmax(predictions, axis=1) \n",
    "y_test_not_one_hot = np.argmax(y_test, axis=1)\n",
    "np.sum((y_predicted_not_one_hot == y_test_not_one_hot) / len(y_test_not_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(13):\n",
    "    subset = df_us[df_us['num_hours']==i]\n",
    "    y = to_categorical(subset['class'])\n",
    "    X = subset[['temperature', 'humidity', 'mslp', 'total_cloud_cover', 'high_cloud_cover', \\\n",
    "                'medium_cloud_cover','low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', \\\n",
    "                'wind_speed_10m', 'wind_dir_10m', 'wind_speed_80m', 'wind_dir_80m', \\\n",
    "                'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, \\\n",
    "                                                        random_state=rs)\n",
    "    \n",
    "    X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    X_scaled_training = X_scaler.fit_transform(X_train)\n",
    "    X_scaled_testing = X_scaler.transform(X_test)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, activation='relu', input_shape=(n_inputs, )))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    adam = keras.optimizers.Adam(0.001)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', categorical_accuracy])\n",
    "    model.fit(X_scaled_training, y_train, epochs=100, verbose=0)\n",
    "    \n",
    "    train_loss = model.evaluate(X_scaled_training, y_train)[0]\n",
    "    train_cat_acc = model.evaluate(X_scaled_training, y_train)[2]\n",
    "    \n",
    "    test_loss = model.evaluate(X_scaled_testing, y_test)[0]\n",
    "    test_cat_acc = model.evaluate(X_scaled_testing, y_test)[2]\n",
    "    \n",
    "    tmp.append([i, train_loss, train_cat_acc, test_loss, test_cat_acc])\n",
    "\n",
    "NN_model_performances = pd.DataFrame(tmp)\n",
    "NN_model_performances.columns = ['num_hours', 'train_loss', 'train_cat_acc', 'test_loss', 'test_cat_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "plt.plot(NN_model_performances.num_hours, NN_model_performances.train_loss, label='Training Loss')\n",
    "plt.plot(NN_model_performances.num_hours, NN_model_performances.test_loss, label='Testing Loss')\n",
    "plt.plot(NN_model_performances.num_hours, NN_model_performances.train_cat_acc, label='Training Categorical Accuracy')\n",
    "plt.plot(NN_model_performances.num_hours, NN_model_performances.test_cat_acc, label='Testing Categorical Accuracy')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Number of Hours to Prediction')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.title('Neural Network Model Performances at Different Prediction Time Intervals');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_layer_options = [1, 2, 3]\n",
    "num_nodes_options = [50, 100, 200]\n",
    "max_epochs = 200\n",
    "patience = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "tmp = []\n",
    "for num_nodes in num_nodes_options:\n",
    "    \n",
    "    for i in range(len(num_hidden_layer_options)):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(num_nodes, activation='relu', input_shape=(n_inputs, )))   \n",
    "        for j in range(num_hidden_layer_options[i]-1):\n",
    "            model.add(Dense(num_nodes, activation='relu'))\n",
    "        \n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        adam = keras.optimizers.Adam(learning_rate)\n",
    "        model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', categorical_accuracy])\n",
    "        early_stopping_monitor = EarlyStopping(patience=patience)\n",
    "        model_hist = model.fit(X_scaled_training, y_train, epochs=max_epochs, \\\n",
    "                               validation_split = 0.3, callbacks=[early_stopping_monitor], verbose=0)\n",
    "        \n",
    "        epochs = early_stopping_monitor.stopped_epoch\n",
    "        \n",
    "        loss = model_hist.history['loss'][-1]\n",
    "        val_loss = model_hist.history['val_loss'][-1]\n",
    "        \n",
    "        cat_acc = model_hist.history['categorical_accuracy'][-1]\n",
    "        val_cat_acc = model_hist.history['val_categorical_accuracy'][-1]\n",
    "        \n",
    "        tmp.append([num_nodes, num_hidden_layer_options[i], epochs, loss, \\\n",
    "                    val_loss, cat_acc, val_cat_acc])\n",
    "        \n",
    "\n",
    "NN_model_tuning = pd.DataFrame(tmp)\n",
    "NN_model_tuning.columns = ['num_nodes', 'num_hidden_layers', 'epochs', 'loss', \\\n",
    "                           'val_loss', 'cat_acc', 'val_cat_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_model_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rain and Snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df = df_us[df_us['class']==1]\n",
    "\n",
    "X = rain_df[rain_df['num_hours']==0].drop(columns=['class', 'num_hours', 'snowfall_hr', 'rainfall_hr', \\\n",
    "                                                   'year', 'month', 'day', 'hour']).values\n",
    "y = rain_df[rain_df['num_hours']==0]['rainfall_hr'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=rs)\n",
    "\n",
    "X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#y_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "X_scaled_training = X_scaler.fit_transform(X_train)\n",
    "#y_scaled_training = y_scaler.fit_transform(y_train)\n",
    "\n",
    "X_scaled_testing = X_scaler.transform(X_test)\n",
    "#y_scaled_testing = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_scaled_training, y_train)\n",
    "lin_reg.score(X_scaled_testing, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape = (n_inputs,)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "adam = keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "model.fit(X_scaled_training, y_train, epochs=200, validation_split=0.3)\n",
    "predictions = model.predict(X_scaled_testing)\n",
    "r2_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=100\n",
    "print(predictions[i])\n",
    "print(y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('basel_weather_7-1-09_to_7-1-19.csv', sep=';', header=10)\n",
    "df = df.drop(columns=['Minute'])\n",
    "column_names = ['year', 'month', 'day', 'hour', 'temperature', 'humidity', 'mslp', 'rainfall_hr', 'rainfall_lr', \\\n",
    "                'snowfall_hr', 'snowfall_lr', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']\n",
    "df.columns = column_names\n",
    "df = df.drop(columns=['snowfall_lr', 'rainfall_lr'])\n",
    "df['hour'] = df['hour'].map(lambda x: (x + 2) % 24)\n",
    "for i, row in df.iterrows():\n",
    "      if row.hour == 0 or row.hour == 1:\n",
    "        df.at[i,'day'] += 1\n",
    "\n",
    "df['class_0hr'] = df.apply(lambda x: make_classes_extra(x.rainfall_hr, x.snowfall_hr), axis = 1)\n",
    "for i in range(1, 13):\n",
    "    col_name = 'class_' + str(i) + 'hr'\n",
    "    shiftnum = i * -1\n",
    "    df[col_name] = df['class_0hr'].shift(shiftnum)\n",
    "    df = df.dropna()\n",
    "    df[col_name] = df[col_name].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class_12hr'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for i in range(0, 13):\n",
    "    col_name = 'class_' + str(i) + 'hr'\n",
    "    cols = ['year', 'month', 'day', 'hour', 'temperature', 'humidity', 'mslp', 'rainfall_hr', \\\n",
    "            'snowfall_hr', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "            'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "            'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust', col_name]\n",
    "    tmp_df = df[cols]\n",
    "    tmp_df = tmp_df.rename(index=str, columns={col_name: 'class'})\n",
    "    df_class_0 = tmp_df[tmp_df['class'] == 0]\n",
    "    df_class_1 = tmp_df[tmp_df['class'] == 1]\n",
    "    df_class_2 = tmp_df[tmp_df['class'] == 2]\n",
    "    df_class_3 = tmp_df[tmp_df['class'] == 3]\n",
    "    df_class_4 = tmp_df[tmp_df['class'] == 4]\n",
    "    \n",
    "    count_class_0, count_class_2, count_class_1, count_class_4, count_class_3 = tmp_df['class'].value_counts()\n",
    "    df_under_0 = df_class_0.sample(count_class_3)\n",
    "    df_under_1 = df_class_1.sample(count_class_3)\n",
    "    df_under_2 = df_class_2.sample(count_class_3)\n",
    "    df_under_4 = df_class_4.sample(count_class_3)\n",
    "    class_df = pd.concat([df_under_0, df_under_1, df_under_2, df_class_3, df_under_4], axis=0).reset_index(drop=True)\n",
    "    class_df['num_hours'] = i\n",
    "    df_list.append(class_df)\n",
    "\n",
    "df_us_extra = pd.concat(df_list).reset_index(drop=True)\n",
    "df_us_extra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_extra['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be: 503 * 5 * 13\n",
    "len(df_us_extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_undersample = df_us_extra[df_us_extra['num_hours']==0]['class'].values\n",
    "y_undersample_cat = to_categorical(df_us_extra[df_us_extra['num_hours']==0]['class'])\n",
    "X_undersample = df_us_extra[df_us_extra['num_hours']==0][['temperature', 'humidity', 'mslp', \\\n",
    "                'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample, test_size=0.3, \\\n",
    "                                                        random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_undersample = df_us_extra[df_us_extra['num_hours']==0]['class'].values.reshape(-1,)\n",
    "y_undersample_cat = to_categorical(df_us_extra[df_us_extra['num_hours']==0]['class'])\n",
    "X_undersample = df_us_extra[df_us_extra['num_hours']==0][['temperature', 'humidity', 'mslp', \\\n",
    "                'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample_cat, test_size=0.3, \\\n",
    "                                                        random_state=rs)\n",
    "\n",
    "X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "X_scaled_training = X_scaler.fit_transform(X_train)\n",
    "X_scaled_testing = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(150, activation='relu', input_shape = (n_inputs,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "adam = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', categorical_accuracy])\n",
    "model.fit(X_scaled_training, y_train, epochs=100, validation_split=0.3)\n",
    "predictions = model.predict(X_scaled_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_rainfall(rainfall_hr):\n",
    "    if rainfall_hr < 0.3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "rain_df = rain_df.drop(columns=['class'])\n",
    "rain_df['class'] = rain_df['rainfall_hr'].apply(categorize_rainfall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rain_df[rain_df['num_hours']==0].drop(columns=['class', 'num_hours', 'snowfall_hr', 'rainfall_hr', \\\n",
    "                                                   'year', 'month', 'day', 'hour']).values\n",
    "y = rain_df[rain_df['num_hours']==0]['class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rain_df[rain_df['num_hours']==0].drop(columns=['class', 'num_hours', 'snowfall_hr', 'rainfall_hr', \\\n",
    "                                                   'year', 'month', 'day', 'hour']).values\n",
    "y = to_categorical(rain_df[rain_df['num_hours']==0]['class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_inputs,)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "adam = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', categorical_accuracy])\n",
    "model.fit(X_train, y_train, epochs=100, validation_split=0.3)\n",
    "predictions = model.predict(X_scaled_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
