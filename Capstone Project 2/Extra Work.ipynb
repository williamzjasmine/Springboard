{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rain and Snow\n",
    "\n",
    "rain_df = df[df['class']==1]\n",
    "\n",
    "X = rain_df[rain_df['num_hours']==0].drop(columns=cols_to_drop).values\n",
    "y = rain_df[rain_df['num_hours']==0]['rainfall_hr'].values.reshape(-1,1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3, random_state=rs)\n",
    "\n",
    "rain_df_test = df_test[df_test['class']==1]\n",
    "X_test = rain_df_test[rain_df_test['num_hours']==0].drop(columns=cols_to_drop).values\n",
    "y_test = rain_df_test[rain_df_test['num_hours']==0]['rainfall_hr'].values.reshape(-1,1)\n",
    "\n",
    "#X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#y_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "#X_scaler = StandardScaler()\n",
    "#y_scaler = StandardScaler()\n",
    "\n",
    "#X_train = X_scaler.fit_transform(X_train)\n",
    "#y_train = y_scaler.fit_transform(y_train)\n",
    "\n",
    "#X_val = X_scaler.transform(X_val)\n",
    "#y_val = y_scaler.transform(y_val)\n",
    "\n",
    "#X_test = X_scaler.transform(X_test)\n",
    "#y_test = y_scaler.transform(y_test)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "lin_reg.score(X_val, y_val)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(10, activation='relu', input_shape = (num_inputs,)))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "#model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "adam = keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=adam, loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val))\n",
    "predictions = model.predict(X_test)\n",
    "r2_score(y_test, predictions)\n",
    "\n",
    "i=2\n",
    "print(predictions[i])\n",
    "print(y_test[i])\n",
    "\n",
    "df = pd.read_csv('basel_weather_7-1-09_to_7-1-19.csv', sep=';', header=10)\n",
    "df = df.drop(columns=['Minute'])\n",
    "column_names = ['year', 'month', 'day', 'hour', 'temperature', 'humidity', 'mslp', 'rainfall_hr', 'rainfall_lr', \\\n",
    "                'snowfall_hr', 'snowfall_lr', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']\n",
    "df.columns = column_names\n",
    "df = df.drop(columns=['snowfall_lr', 'rainfall_lr'])\n",
    "df['hour'] = df['hour'].map(lambda x: (x + 2) % 24)\n",
    "for i, row in df.iterrows():\n",
    "      if row.hour == 0 or row.hour == 1:\n",
    "        df.at[i,'day'] += 1\n",
    "\n",
    "df['class_0hr'] = df.apply(lambda x: make_classes_extra(x.rainfall_hr, x.snowfall_hr), axis = 1)\n",
    "for i in range(1, 13):\n",
    "    col_name = 'class_' + str(i) + 'hr'\n",
    "    shiftnum = i * -1\n",
    "    df[col_name] = df['class_0hr'].shift(shiftnum)\n",
    "    df = df.dropna()\n",
    "    df[col_name] = df[col_name].astype(int)\n",
    "\n",
    "\n",
    "df['class_12hr'].value_counts()\n",
    "\n",
    "\n",
    "df_list = []\n",
    "for i in range(0, 13):\n",
    "    col_name = 'class_' + str(i) + 'hr'\n",
    "    cols = ['year', 'month', 'day', 'hour', 'temperature', 'humidity', 'mslp', 'rainfall_hr', \\\n",
    "            'snowfall_hr', 'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "            'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "            'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust', col_name]\n",
    "    tmp_df = df[cols]\n",
    "    tmp_df = tmp_df.rename(index=str, columns={col_name: 'class'})\n",
    "    df_class_0 = tmp_df[tmp_df['class'] == 0]\n",
    "    df_class_1 = tmp_df[tmp_df['class'] == 1]\n",
    "    df_class_2 = tmp_df[tmp_df['class'] == 2]\n",
    "    df_class_3 = tmp_df[tmp_df['class'] == 3]\n",
    "    df_class_4 = tmp_df[tmp_df['class'] == 4]\n",
    "    \n",
    "    count_class_0, count_class_2, count_class_1, count_class_4, count_class_3 = tmp_df['class'].value_counts()\n",
    "    df_under_0 = df_class_0.sample(count_class_3)\n",
    "    df_under_1 = df_class_1.sample(count_class_3)\n",
    "    df_under_2 = df_class_2.sample(count_class_3)\n",
    "    df_under_4 = df_class_4.sample(count_class_3)\n",
    "    class_df = pd.concat([df_under_0, df_under_1, df_under_2, df_class_3, df_under_4], axis=0).reset_index(drop=True)\n",
    "    class_df['num_hours'] = i\n",
    "    df_list.append(class_df)\n",
    "\n",
    "df_us_extra = pd.concat(df_list).reset_index(drop=True)\n",
    "df_us_extra.head()\n",
    "\n",
    "df_us_extra['class'].value_counts()\n",
    "\n",
    "# Should be: 503 * 5 * 13\n",
    "len(df_us_extra)\n",
    "\n",
    "y_undersample = df_us_extra[df_us_extra['num_hours']==0]['class'].values\n",
    "y_undersample_cat = to_categorical(df_us_extra[df_us_extra['num_hours']==0]['class'])\n",
    "X_undersample = df_us_extra[df_us_extra['num_hours']==0][['temperature', 'humidity', 'mslp', \\\n",
    "                'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample, test_size=0.3, \\\n",
    "                                                        random_state=rs)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n",
    "knn.score(X_test, y_test)\n",
    "\n",
    "y_undersample = df_us_extra[df_us_extra['num_hours']==0]['class'].values.reshape(-1,)\n",
    "y_undersample_cat = to_categorical(df_us_extra[df_us_extra['num_hours']==0]['class'])\n",
    "X_undersample = df_us_extra[df_us_extra['num_hours']==0][['temperature', 'humidity', 'mslp', \\\n",
    "                'total_cloud_cover', 'high_cloud_cover', 'medium_cloud_cover', \\\n",
    "                'low_cloud_cover', 'sunshine_duration', 'shortwave_radiation', 'wind_speed_10m', 'wind_dir_10m',\\\n",
    "                'wind_speed_80m', 'wind_dir_80m', 'wind_speed_900mb', 'wind_dir_900mb', 'wind_gust']].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_undersample, y_undersample_cat, test_size=0.3, \\\n",
    "                                                        random_state=rs)\n",
    "\n",
    "X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "X_scaled_training = X_scaler.fit_transform(X_train)\n",
    "X_scaled_testing = X_scaler.transform(X_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(150, activation='relu', input_shape = (n_inputs,)))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "adam = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', categorical_accuracy])\n",
    "model.fit(X_scaled_training, y_train, epochs=100, validation_split=0.3)\n",
    "predictions = model.predict(X_scaled_testing)\n",
    "\n",
    "def categorize_rainfall(rainfall_hr):\n",
    "    if rainfall_hr < 0.3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "rain_df = rain_df.drop(columns=['class'])\n",
    "rain_df['class'] = rain_df['rainfall_hr'].apply(categorize_rainfall)\n",
    "\n",
    "rain_df['class'].value_counts()\n",
    "\n",
    "X = rain_df[rain_df['num_hours']==0].drop(columns=['class', 'num_hours', 'snowfall_hr', 'rainfall_hr', \\\n",
    "                                                   'year', 'month', 'day', 'hour']).values\n",
    "y = rain_df[rain_df['num_hours']==0]['class'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=rs)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n",
    "knn.score(X_test, y_test)\n",
    "\n",
    "X = rain_df[rain_df['num_hours']==0].drop(columns=['class', 'num_hours', 'snowfall_hr', 'rainfall_hr', \\\n",
    "                                                   'year', 'month', 'day', 'hour']).values\n",
    "y = to_categorical(rain_df[rain_df['num_hours']==0]['class'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=rs)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_shape = (n_inputs,)))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "adam = keras.optimizers.Adam(0.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy', categorical_accuracy])\n",
    "model.fit(X_train, y_train, epochs=100, validation_split=0.3)\n",
    "predictions = model.predict(X_scaled_testing)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#X = df[df['num_hours']==0].drop(columns=cols_to_drop).values\n",
    "#y = to_categorical(df[df['num_hours']==0]['class'])\n",
    "#X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=rs)\n",
    "#sm = SMOTE(random_state=rs)\n",
    "#X_train, y_train = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "#X_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#X_train = X_scaler.fit_transform(X_train)\n",
    "#X_val = X_scaler.transform(X_val)\n",
    "\n",
    "#X_test = df_test[df_test['num_hours']==0].drop(columns=cols_to_drop).values\n",
    "#y_test = df_test[df_test['num_hours']==0]['class'].values.reshape(-1,)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = prepare_data(df, num_hours=0, do_std_scaler=True, \\\n",
    "                                                              do_pca=11, do_smote = True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "num_inputs = X_train.shape[1]\n",
    "num_outputs = 3\n",
    "\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('inupt'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, num_inputs))\n",
    "\n",
    "with tf.variable_scope('layer1'):\n",
    "    weights = tf.get_variable(name='weights1', shape=[num_inputs, layer_1_nodes], \\\n",
    "                              initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_1_out = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "with tf.variable_scope('layer2'):\n",
    "    weights = tf.get_variable(name='weights2', shape=[layer_1_nodes, layer_2_nodes], \\\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_out = tf.nn.relu(tf.matmul(layer_1_out, weights) + biases)\n",
    "\n",
    "with tf.variable_scope('layer3'):\n",
    "    weights = tf.get_variable(name='weights3', shape=[layer_2_nodes, layer_3_nodes], \\\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_out = tf.nn.relu(tf.matmul(layer_2_out, weights) + biases)\n",
    "\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name='weights4', shape=[layer_3_nodes, num_outputs], \\\n",
    "                             initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\", shape=[num_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.softmax(tf.matmul(layer_3_out, weights) + biases)\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "    cost = tf.losses.softmax_cross_entropy(Y, prediction)\n",
    "\n",
    "# define optimzier\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter('./logs/training', session.graph)\n",
    "    testing_writer = tf.summary.FileWriter('./logs/testing', session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_train, Y: y_train})\n",
    "\n",
    "         # Every few training steps, log our progress\n",
    "        if epoch % display_step == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_train, Y:y_train})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_val, Y:y_val})\n",
    "\n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "\n",
    "            # Print the current training status to the screen\n",
    "            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "    # Training is now complete!\n",
    "\n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X: X_train, Y: y_train})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X: X_val, Y: y_val})\n",
    "\n",
    "    print(\"Final Training cost: {}\".format(final_training_cost))\n",
    "    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
    "\n",
    "    y_predicted = session.run(prediction, feed_dict={X: X_val})\n",
    "    \n",
    "    inputs = {\n",
    "        'input': tf.saved_model.utils.build_tensor_info(X)\n",
    "        }\n",
    "    outputs = {\n",
    "        'earnings': tf.saved_model.utils.build_tensor_info(prediction)\n",
    "        }\n",
    "\n",
    "i = 21\n",
    "y_test_not_one_hot = np.argmax(y_val, axis=1)\n",
    "if y_test_not_one_hot[i] == 0:\n",
    "    weather = 'Clear'\n",
    "elif y_test_not_one_hot[i] == 1:\n",
    "    weather = 'Rain'\n",
    "else:\n",
    "    weather = 'Snow'\n",
    "\n",
    "print('Sample Prediction from Neural Network:')\n",
    "print('Percent Chance of Clear Weather: {}'.format(y_predicted[i][0]*100))\n",
    "print('Percent Chance of Rain: {}'.format(y_predicted[i][1]*100))\n",
    "print('Percent Chance of Snow: {}'.format(y_predicted[i][2]*100))\n",
    "print('\\n\\nAcutal Weather: %s' %weather)\n",
    "\n",
    "y_predicted_not_one_hot = np.argmax(y_predicted, axis=1) \n",
    "print('Neural Network Validation Set Accuracy: {}'\\\n",
    "      .format(np.sum((y_predicted_not_one_hot == y_test_not_one_hot) / len(y_test_not_one_hot))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
